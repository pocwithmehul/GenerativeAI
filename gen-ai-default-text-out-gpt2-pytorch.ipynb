{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200ce48c-d782-43e4-8084-b156ae7944be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\"), \"CUDA\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\"), \"MPS\"\n",
    "    else:\n",
    "        return torch.device(\"cpu\"), \"CPU\"\n",
    "\n",
    "def print_device_info(device_type):\n",
    "    if device_type == \"CUDA\":\n",
    "        num_gpus = torch.cuda.device_count()\n",
    "        print(f\"Using CUDA. Number of GPUs available: {num_gpus}\")\n",
    "    elif device_type == \"MPS\":\n",
    "        print(\"Using MPS (Apple Silicon GPU).\")\n",
    "    else:\n",
    "        num_cpus = torch.get_num_threads()\n",
    "        print(f\"Using CPU. Number of CPUs available: {num_cpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee20f515-ff7c-4c45-bdcd-7a6f7cc4901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU).\n"
     ]
    }
   ],
   "source": [
    "device, device_type = get_device()\n",
    "print_device_info(device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18de8f50-5f5b-40b5-80a9-b0ddecc65462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize tokenizer and model with the specified device\n",
    "model_name = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f4d715e-b395-4cc4-ba5b-6ea926fda83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a1c8e98-35ea-4761-be1f-8afa1e84e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487379b0-a978-406f-9ac4-725cb9a0e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to generate response: 3.05 seconds\n"
     ]
    }
   ],
   "source": [
    "# Generate text based on a prompt\n",
    "prompt = \"When is the best time to visit Yosemite\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "generated_outputs = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "# Calculate and print the time taken\n",
    "end_time = time.time()\n",
    "print(f\"Time taken to generate response: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3aa29b6-90fa-4ef0-9140-ef3d7e86020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When is the best time to visit Yosemite?\n",
      "\n",
      "The best time to visit Yosemite is when you're ready to go.\n",
      "\n",
      "The best time to visit Yosemite is when you're ready to go.\n",
      "\n",
      "The best time to visit Yosemite is\n"
     ]
    }
   ],
   "source": [
    "# Decode and print the generated text\n",
    "generated_text = tokenizer.decode(generated_outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691188df-219c-4c4f-b0d5-745bd4600a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830c822c-1d77-4d12-9212-5393149f0bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
